## 1. Hadoop配置snappy压缩，具体参考说明文档


## 2. 开启Map和reduce输出阶段压缩
### 2.1 map输出节点压缩可以减少job中map和reduce task之间数据传输量
- a. 开启hive中间传输数据压缩功能
```shell
hive> set hive.exec.compress.intermediate=true;
```
- b. 开启mapreduce中map输出压缩功能
```shell
hive> set mapreduce.map.output.compress=true;
```
- c. 设置mapreduce中map输出数据的压缩方式
```shell
hive> set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;
hive> select sum(score) from score; # 可以通过WebUI查看任务history中的configuration查看该job的压缩属性信息
```

### 2.2 Hive将输出写入表中，输出内容同样可以进行压缩。通过属性hive.exec.compress.output控制。
- a. 开启hive最终输出数据压缩功能
```shell
hive> set hive.exec.compress.output=true;
```
- b. 开启mapreduce最终输出数据压缩功能
```shell
hive> set mapreduce.output.fileoutputformat.compress=true;
```
- c. 设置mapreduce最终数据压缩输出方式
```shell
hive> set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;
```
- e. 设置mapreduce最终输出数据压缩为块压缩
```shell
hive> set mapreduce.output.fileoutputformat.compress.type=BLOCK;
hive> insert overwrite local directory '/home/ws/module/hive/data/tmp'
    > select * from score distribute by subject sort by score desc;
```


## 3. 文件存储格式
### 3.1 列式存储和行式存储
- a. 行存储：查询满足条件的一整行数据的时候，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询速度更快
- b. 列存储：因为每个字段数据的聚集存储，在查询时只需要少数几个字段的时候，能大大减少读取的数据量。每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。
- c. TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；ORC和PARQUET是基于列式存储的

### 3.2 TextFile格式
- a. 默认格式，数据不做压缩，磁盘开销和数据解析开销大。可结合gzip,bzip2使用。
- b. 注意：使用gzip格式，hive不会对数据进行切分，进而无法对数据进行并行操作
```shell script
# 可以直接将gzip、bzip2压缩格式导入到textfile存储格式的表中，hive会自动识别压缩类型并进行解压
> create table raw (line string) row format delimited fields terminated by '\t' lines terminated by '\n';
> load data local inpath '/tmp/a.gz' into table raw; 

# 针对gzip格式hive不做数据拆分的问题，推荐的方法是插入数据到另一个表，存储格式为SequenceFile
> create table raw_sequence (line string) stored as sequencefile;
> set hive.exec.compress.output=true;
# 指定压缩方式：RECORD会压缩每个值，而BLOCK在压缩前会缓存1M
> set io.seqfile.compression.type=BLOCK; # NONE/RECORD/BLOCK
> insert overwrite table raw_sequence select * from raw;
```

### 3.3 ORC：默认的压缩格式为zlib
- (1) 使用ORC文件格式，提高Hive的读写数据、处理数据的性能
- (2) 文件结构：![ORC文件结构图](../../note/pictures/8.%20OrcFileLayout.png)
- a. stripes: 原始数据组，每个stripe默认大小为250M。每个stripe中可划分为索引数据(index data), row data, stripe footer  
其中：  
  stripe footer: 包含流位置的目录
  row data: 用于表扫描  
  index data: 包含每列的最大最小值，以及每列中行位置。行索引似的在较大的stripe中也可以执行row-skipping。默认可跳跃10000行  
- b. file footer: 辅助信息，包含文件中的stripe列表，每个stripe的行数，每个列的数据结构。也包含count, max, min, sum的聚合信息
- c. postsrcipt: 包含压缩参数以及压缩脚本的大小
- (3) 指定ORC文件格式
```shell script
> create table ... stored as orc;
> alter table ... [partition partition_orc] set fileformat orc;
> set hive.default.fileformat=orc;
# 创建orc表
create table Addresses (
  name string,
  street string,
  city string,
  state string,
  zip int
) stored as orc tblproperties ("orc.compress"="NONE");
```
(4) 使用hive --orcfiledump可以对orc文件进行分析
- 参考：
- ![LanguageManual ORC](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC)
- ![Hive-ORC文件存储格式](https://blog.csdn.net/dabokele/article/details/51813322)
- ![Hive数据导入方案—使用ORC格式存储hive数据](https://blog.csdn.net/javastart/article/details/50992851)
```shell script
> hive --orcfiledump hdfs://path/orc # 分析orc文件
# hive --orcfiledump [-j] [-p] [-d] [-t] [--rowindex <col_ids>] [--recover] [--skip-dump] [--backup-path <new-path>] <location-of-orc-file-or-directory>
# -d： dump orc文件数据而非其metadata
```

### 3.4 Parquet
```shell

测试：
a. 创建不同文件存储格式的表，比对hive中文件的大小
# 创建textfile格式
hive> create table if not exists log_text(track_time string, url string, session_id string, referer string, ip string,
    > end_user_id string, city_id string) row format delimited fields terminated by '\t' stored as textfile ;
hive> load data local inpath '/home/ws/module/hive/data/log.data' into table log_text; # 存储后的大小为18.13M
hive> select * from log_text sort by city_id limit 100;    # 查询耗时103.102s
# 创建orc格式
hive> create table if not exists log_text(track_time string, url string, session_id string, referer string, ip string,
    > end_user_id string, city_id string) row format delimited fields terminated by '\t' stored as ocr;
hive> insert into table log_orc select * from log_text; # 存储后的文件大小为2.78M
hive> select * from log_orc sort by city_id limit 100;    # 查询耗时0.138s
# 创建parquet格式
hive> create table if not exists log_parquet(track_time string, url string, session_id string, referer string, ip string,
    > end_user_id string, city_id string) row format delimited fields terminated by '\t' stored as parquet;
hive> insert into table log_parquet select * from log_text; # 存储后的文件大小为13.09M
hive> select * from log_parquet sort by city_id limit 100;    # 查询耗时0.141s
```

## 4. 测试存储和压缩
```shell
# 创建非压缩的ORC存储方式
hive> create table if not exists log_orc_none(track_time string, url string, session_id string, referer string, ip string,
    > end_user_id string, city_id string) row format delimited fields terminated by '\t' stored as orc tblproperties("orc.compress"="NONE");
# 插入数据，存储后的大小为7.69 M
hive> insert overwrite table log_orc_none select * from log_text;
# 创建一个snappy压缩的ORC存储方式
hive> create table if not exists log_orc_snappy(track_time string, url string, session_id string, referer string, ip string,
    > end_user_id string, city_id string) row format delimited fields terminated by '\t' stored as orc tblproperties("orc.compress"="SNAPPY");
# 插入数据，存储后的大小为3.75 M
hive> insert overwrite table log_orc_snappy select * from log_text;

```

## 5. 在实际的项目开发中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy, lzo

## 6. Avro格式
```shell script
# 创建textfile存储类型的表
CREATE TABLE test_serializer(string1 STRING,
                             int1 INT,
                             tinyint1 TINYINT,
                             smallint1 SMALLINT,
                             bigint1 BIGINT,
                             boolean1 BOOLEAN,
                             float1 FLOAT,
                             double1 DOUBLE,
                             list1 ARRAY<STRING>,
                             map1 MAP<STRING,INT>,
                             struct1 STRUCT<sint:INT,sboolean:BOOLEAN,sstring:STRING>,
                             union1 uniontype<FLOAT, BOOLEAN, STRING>,
                             enum1 STRING,
                             nullableint INT,
                             bytes1 BINARY,
                             fixed1 BINARY)
 ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' COLLECTION ITEMS TERMINATED BY ':' MAP KEYS TERMINATED BY '#' LINES TERMINATED BY '\n'
 STORED AS TEXTFILE;

# 数据如下
# why hello there,42,3,100,1412341,true,42.43,85.23423442,alpha:beta:gamma,Earth#42:Control#86:Bob#31,17:true:Abe Linkedin,0:3.14159,BLUE,72,^A^B^C,^A^B^C

# 插入数据
load data local inpath '/tmp/a.txt' into table test_serializer;

# 创建avro存储格式的表
CREATE TABLE as_avro(string1 STRING,
                     int1 INT,
                     tinyint1 TINYINT,
                     smallint1 SMALLINT,
                     bigint1 BIGINT,
                     boolean1 BOOLEAN,
                     float1 FLOAT,
                     double1 DOUBLE,
                     list1 ARRAY<STRING>,
                     map1 MAP<STRING,INT>,
                     struct1 STRUCT<sint:INT,sboolean:BOOLEAN,sstring:STRING>,
                     union1 uniontype<FLOAT, BOOLEAN, STRING>,
                     enum1 STRING,
                     nullableint INT,
                     bytes1 BINARY,
                     fixed1 BINARY)
STORED AS AVRO;

# 插入数据
INSERT OVERWRITE TABLE as_avro SELECT * FROM test_serializer;
```