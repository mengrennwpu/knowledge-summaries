## 1. 概念
Hive是数据仓库软件，用于读取、写入以及管理分布式存储的大型数据集，并提供SQL语法进行查询。
## 2. 特征
- (1) 提供了SQL轻松访问数据的工具，进而实现诸如ETL、报告及数据分析等的数据仓库任务
- (2) 提供了易于处理各种数据格式的机制
- (3) 能够直接访问HDFS中的数据，以及其他数据存储系统，如HBase
- (4) 可通过Tez, Spark或MR执行查询
- (5) 提供了HPL-SQL的过程语言
- (6) 通过Hive LLAP, Yarn和Apache Slider可进行亚秒级查询检索  

Hive SQL也可以通过用户代码如UDFs(user defined functions), UDAFs(user defined aggregate functions), UDTFs(user defined table functions)进行扩展。

Hive带有内置连接器，用于逗号分隔或制表符分隔值(CSV/TSV)的文本文件, Apache Parquet, Apache ORC或其他格式。
## 3. 组件
### HCatalog
Hadoop的表和存储的管理层，使得使用不同数据处理工具(Pig或MR)的用户能够更轻松地在网格上读写数据
### WebHCat
提供了可用于运行MR、Yarn、Pig、Hive作业的服务。还可以使用HTTP接口执行Hive元数据操作

## 4. 运行Hive
- (1) 通过hive: $HIVE_HOME/bin/hive
- (2) 通过HiveServer2和beeline:   
	  `$ $HIVE_HOME/bin/hiveserver2`  
	  `$ $HIVE_HOME/bin/beeline -u jdbc:hive2://localhost:10000 -n test`

## 5. local模式
大多数HadoopJob需要hadoop提供完整的可扩展性来处理大数据集。不过，有时hive的输入数据量非常小。此时，为查询触发执行任务消耗的时间可能会比实际job执行时间更长。对于这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显缩短。  
`hive> set hive.exec.mode.local.auto=false;`  
该设置默认为false，当设为true时，Hive会分析查询中每个map-reduce作业的大小，且满足如下条件中的一个，将会以本地模式运行。  
(1) 作业总输入大小小于配置hive.exec.mode.local.auto.inputbytes.max(默认为128M)  
(2) map任务的总数小于配置hive.exec.local.auto.tasks.max(默认为4)  
(3) reduce任务的个数为1或者0  
- 注意：local模式的执行实在Hive Client中的子JVM中执行的，可以通过hive.mapred.local.mem进行内存限制，默认为0，即Hadoop决定该子JVM的内存限制。

## 6. 日志
- (1) `查询`日志默认会存储在/tmp/<user.name>/下，可以在hive-site.xml中通过hive.querylog.location属性配置
- (2) 通过设置hive.log.explain.output为true，可以让查询的EXPLAIN EXTENDED输出以INFO级别进行记录

## 7. DDL操作
### (1) 创建表
`hive> create table pokes(foo int, bar string) row format delimited fields terminated by '\t' # 字段默认以'\t'分隔`
`hive> create table invites(foo int, bar string) partitioned by (ds string) row format delimited fields terminated by '\t';`

### (2) 显示表信息
- 显示所有以"s"结尾的表  
`hive> show tables '.*s';`
- 显示表的列信息  
`hive> desc invites;`

### (3) 更改表信息
- 更改表名  
`hive> alter table events rename to 3koobecaf;`  
`hive> alter table invites add columns (new_col int comment 'a comment');`  
`hive> alter table invites replace columns(foo int, bar string, baz int  comment 'baz replaces new_col');`
- 注意：replace columns会替换所有的列

### (4) 删除表
`hive> drop table pokes;`

## 8. DML操作
### （1） 创建表
`#加载本地数据到指定hive表中`
`hive> load data local inpath './data/pokes.txt' overwrite into table pokes; `

`加载本地数据到指定表的分区(默认为追加)`
`hive> load data local inpath './data/pokes.txt' into table invites partition (ds=20200622); # `
`hive> load data local inpath './data/pokes.txt' into table invites partition (ds=20200623);`

`加载本地数据到指定表的分区(指定为覆盖)`
`hive> load data local inpath './data/pokes.txt' into table invites partition (ds=20200623);`

- 注意：从HDFS中加载数据，默认会导致对应数据的移动。

### (2) SQL操作
#### 1) select和filter
`hive> select a.foo from invites a where a.ds='20200623';`
- 将查询结果保存在HDFS目录中(目录下的文件个数依赖于mapper的个数)
`hive> insert overwrite directory '/tmp/hdfs_out' select a.* from invites a where a.ds=20200622;`
- 将查询结果保存在本地目录中
`hive> insert overwrite local directory '/tmp/local_out' select a.* from invites a;`

- 其他示例：
`hive> insert overwrite table events select a.* from pokes a;`
`hive> insert overwrite table events select a.* from pokes a where a.foo > 2;`
`hive> insert overwrite local directory '/tmp/reg_3' select a.* from pokes a;`
`hive> insert overwrite directory '/tmp/reg_4' select a.foo, a.bar from pokes a;`
`hive> insert overwrite directory '/tmp/reg_5' select count(*) from invites a where a.ds=20200622;`
`hive> insert overwrite directory '/tmp/reg_5' select a.foo, a.bar from invites a;`
`hive> insert overwrite local directory '/tmp/sum' select sum(a.foo) from pokes a;`

#### 2) group by
`hive> from invites a insert overwrite table events select a.bar, count(*) where a.foo > 0 group by a.bar;`

#### 3) join
`hive> from pokes t1 join invites t2 on (t1.bar=t2.bar) insert overwrite table events select t1.bar, t1.foo, t2.foo;`

#### 4) 多表插入
```
FROM src
insert overwrite table dest1 select src.* where src.key < 100
insert overwrite table dest2 select src.key,src.value where src.key >=100 and src.key < 200
insert overwrite table dest3 partition(ds=20200620) select src.key where src.key >= 200 and src.key < 300
insert overwrite local directory '/tmp/dest4.out' select src.value where src.key >= 300;
```

#### 5) streaming
- 参考文章：![Hive的Transform功能](https://www.cnblogs.com/aquastone/p/hive-transform.html)
`hive> from invites a insert overwrite table events select transform(a.foo, a.bar) as (oof, rab) using '/bin/cat' where a.ds > '20200622';`

- 在reduce阶段对每天的uid形成一个列表，进行排序并输出  
```
from {  
	from pv_users  
	select transform(pv_users.userid, pv_users.data)  
	using 'map_script'  
	as (dt, uid)  
	cluster by dt  
} map_output  
insert overwrite table pv_users_reduced  
select transform(map_output.dt, map_output.uid)  
using 'reduce_script'  
as date, count;  
```
- 代码的大致流程如下：
map_script作为mapper，reduce_script作为reducer。将pv_users表中的userid, date两列作为mapper的输入字段，处理后的输出的前两个字段分别命名为dt, uid，并按照dt字段做partition和sort送给reduce阶段处理。reduce的输入字段为dt和uid，输出处理后的前两个字段，并命名为data, count，写入到pv_users_reduced表中。

## 9. 简单用例：

### (1) MovieLens用户评分

- 创建表：
```
create table u_data(
	userid int,
	movieid int,
	rating int,
	unixtime string)
row format delimited
fields terminated by '\t'
stored as textfile; 
```
- 加载数据
`hive> load data local inpath 'data/ml-100k/u.data' overwrite into table u_data;`

- 查询总量
`hive> select count(*) from u_data;`

- 复杂操作
- a. 创建python脚本weekday_mapper.py 
```python
import sys
import datetime

for line in sys.stdin:
    line = line.strip()
    userid, movieid, rating, unixtime = line.split('\t')
    weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()
    print '\t'.join([userid, movieid, rating, str(weekday)])

```
- b. 创建新表
```
create table u_data_new (
	userid int,
	movieid int,
	rating int,
	weekday int)
row format delimited
fields terminated by '\t';
```
- c. 将脚本加入hive中
```add file weekday_mapper.py```

- d. u_data的数据通过调用脚本传入到u_data_new
```
insert overwrite table u_data_new
select
	transform (userid, movieid, rating, unixtime)
	using 'python weekday_mapper.py'
	as (userid, movieid, rating, weekday)
from u_data;
```

- e. 查询
```
select weekday, count(*)
from u_data_new
group by weekday;
```

### (2) Appche Weblog Data
```
# 创建Apache web日志默认格式对应的表结构
create table apachelog (
	host string,
	identity string,
	user string,
	time string,
	request string,
	status string,
	size string,
	referer string,
	agent string)
row format serde 'org.apache.hadoop.hive.serde2.RegexSerDe'
with serdeproperties (
	"input.regex" = "([^]*) ([^]*) ([^]*) (-|\\[^\\]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\".*\") ([^ \"]*|\".*\"))?"
)
stored as textfile;
```