## 1. 创建数据库
```shell
# 默认路径为/user/hive/warehouse/hive_db.db
hive> create database if not exists hive_db;

# 在hive_db下创建test表
hive> create table if not exists hive_db.test(id int);

# 在指定HDFS中路径下创建库
hive> create database if not exists db_hive2 location '/hive_db2.db'; 
```


## 2. 查询数据库
```shell
# 查看所有库
hive> show databases; 

# 查看hive开头的库
hive> show databases like 'hive*'; 

# 查看hive_db库的信息
hive> desc database hive_db;

# 查看hive_db库的详情
hive> desc database extended hive_db;

# 查看当前使用的表名
hive> select current_database();
```


## 3. 修改数据库
- 使用ALTER DATABASE为某个数据库的DBPROPERTIES设置键值对属性。
- 注意:
- a.数据库的其他元数据信息不可更改，包括数据库名和数据库所在的目录位置。
- b. 修改当前正在使用的数据库，要先退出使用
```shell    
hive> alter database hive_db set dbproperties('createtime'='20191121');
# 不会更改已存在数据的默认路径，只会更改新表的默认路径
hive> alter database hive_db set location hdfs_path;
hive> desc database extended hive_db;
```


## 4. 删除数据库
```shell
# 当hive_db2库下没有表，可以执行删除
hive> drop database if exists hive_db2; 
hive> drop database if exists hvie_db2 restrict; # 与上式等同，默认是restrict

# 当hive_db下有表时，采用级联删除
drop database if exists hive_db cascade;
```


## 5. 创建表的格式如下
```
CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
    [(col_name data_type [COMMENT col_comment], ...)]
    [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
    [CLUSTERED BY (col_name, col_name, ...)
        [SORT BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
    [ROW FORMAT row_format]
    [STORED AS file_format]
    [LOCATION hdfs_path]

其中：
    a. PARTITIONED BY：创建分区表，针对文件夹
    b. CLUSTERED BY: 创建分桶表，针对文件，相当于MR中的分区
```
```shell
# 创建表
hive (default)> create table page_view(
              >   viewTime int,
              >   userid bigint,
              >   page_url string,
              >   referer_url string,
              >   ip string comment 'ip address of the user')
              > comment 'this is the page view table'
              > partitioned by (dt string, country string)
              > clustered by (userid) sorted by (viewTime) into 32 buckets
              > row format delimited
              >   fields terminated by '1'
              >   collection items terminated by '2'
              >   map keys terminated by '3'
              > stored as sequencefile;
hive> show tables;
hive> show tables 'page.*';
hive> show partitions page_view;	# 列出表分区
hive> desc page_view;
hive> desc extended page_view;
hive> desc extended page_view partition(ds='2008-08-08');

# Row formats & SerDe
# 1. RegexSerDe
# >... ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' with serdeproperties ("input.regex"="<regex>") stored as textfile;
hive> CREATE TABLE apachelog (
        host STRING,
        identity STRING,
        user STRING,
        time STRING,
        request STRING,
        status STRING,
        size STRING,
        referer STRING,
        agent STRING)
      ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
      WITH SERDEPROPERTIES (
        "input.regex" = "([^]*) ([^]*) ([^]*) (-|\\[^\\]*\\]) ([^ \"]*|\"[^\"]*\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \"]*|\".*\") ([^ \"]*|\".*\"))?"
      )
      STORED AS TEXTFILE;

# 2. JsonSerDe 
# hive 3.0起使用json序列化
# > ... ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.JsonSerDe' stored as textfile;
hive> CREATE TABLE my_table(a string, b bigint, ...)
      STORED AS TEXTFILE;
      ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.JsonSerDe'
# hive 4.0起优化为如下：
                  WITH
hive> create table my_table(a string, b bigint, ...) stored as jsonfile;
a
# 3. CSV/TSV
# > ... row format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde' stored  SERDEPROPERTIES (s textfile;
hive> CREATE TABLE my_table(a string, b string, ...)
      ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
         "separatorChar" = "\t", # 默认为','
         "quoteChar"     = "'", # 默认为'"'
         "escapeChar"    = "\\" # 默认为'\'
      )  
      STORED AS TEXTFILE;

```
    

# 6. 管理表
- 即内部表，Hive会或多或少地控制着数据的生命周期。而与之相对地就是外部表，适于数据共享场景。

### 6.1 管理表：
- Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(如/user/hive/warehouse)所定义目录的子目录下。
- 当我们删除一个管理表时，Hive也会删除这个表中数据。因此管理表不适合和其他工具共享数据。

### 6.2 外部表：
- 当删除一个外部表时，对应的metadata会删除，但HDFS中的表文件夹不会删除。因此metadata与实际表内容独立，数据更加安全
- 当重新创建相同的外部表时，由于HDFS中的文件夹并未删除，因此重新查询时仍然有值。
```shell
    示例：
    a. 管理表
    hive> create table if not exists student(  # 创建表
        > id int, name string)
        > row format delimited terminated by '\t'
        > stored as textfile
        > location '/user/hive/warehouse/student';
    hive> load data local inpath '/home/test/module/hive/data/student*' into table student;
    hive> drop table if exists student; # 删除过后，对应的HDFS对应的表文件也会删除
    b. 外部表
    hive> create external table if not exists student(  # 创建表
        > id int, name string)
        > row format delimited terminated by '\t'
        > stored as textfile
        > location '/user/hive/warehouse/student';
    hive> load data local inpath '/home/test/module/hive/data/student*' into table student;
    hive> drop table if exists student; # 删除过后，对应的HDFS对应的表文件不会删除
```

### 6.3 管理表与外部表的相互转换
```shell
    hive> desc formatted student;   # 查看表的类型 Table Type: MANAGED_TABLE
    hive> alter table student set tblproperties('EXTERNAL'='TRUE'); # 将内部表转换为外部表
    hive> desc formatted student;   # 查看表的类型 Table Type: EXTERNAL_TABLE
    hive> alter table student set tblproperties('EXTERNAL'='FALSE'); # 将外部表转换为内部表
    注意：'EXTERNAL'='TRUE'和'EXTERNAL'='FALSE'为固定写法，且区分大小写
```

## 7. 分区表
- 实际上就是对应HDFS的独立的文件夹，该文件夹是该分区的所有数据文件。Hive分区的目的就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询通过WHERE子句中的表达式选择查询所需要的指定分区，进而提高查询效率。
- 注意：分区的定义字段与表的定义字段不能相同
- 
### 7.1 分区表基本操作
```shell
    # 创建分区表：基于月份进行分区
    hive> create table student_partition(id int, name string)
        > partitioned by(month string)
        > row format delimited fields terminated by '\t';

    # 数据加载进分区表中
    hive> load data local inpath '/home/test/module/hive/data/student*' into table student_partition partition(month='201911');

    # 查看数据
    hive> select * from student_partition; # 显示全部分区数据，可以看到多出来一列：分区指定的列
    hive> select * from student_partition where month=201911; # 查询指定分区的内容
    hive> select * from student_partition where month='201912' or month='201911'; # 组合查询1
    hive> select * from student_partition where month='201912'; # 组合查询2
        > union select * from student_partition where month='201911';
    hive> desc formatted student_partition; # 可以看到分区信息下的显示信息

    # 针对一个分区表增加分区
    hive> alter table student_partition add partition(month='202001') partition(month='202002'); # 注意多个分区之间用空格

    # 删除分区表的分区
    hive> alter table student_partition drop partition(month='202001'),partition(month='202002'); # 注意多个分区之间用逗号

    # 查看分区表的分区个数
    hive> show partitions student_partition;
```

### 7.2 创建二级分区
```shell
    hive> create table if not exists student2(id int, name string)
        > partitioned by (month string, day string) # 二级分区
        > row format delimited fields terminated by '\t';
    hive> load data local inpath '/home/test/module/hive/data/student*' into table student2
        > partition(month='202001',day='01'); # 加载数据
    hive> select * from student2;
```

### 7.3 分区表与数据关联的三种方式：
- 手动创建修复：当在HDFS上新建分区目录并上传文件到分区目录上，hive中查询该分区表的内容信息将为空，原因是分区的元数据没有创建，此时需要执行修复命令
```shell
    示例：
    hive> dfs -mkdir -p /user/hive/warehouse/hive_test.db/student_partition/month=202001; # 创建目录
    hive> dfs -put /home/test/module/hive/data/student.txt /user/hive/warehouse/hive_test.db/student_partition/month=202001; # 上传文件
    hive> select * from student_partition where month='202001';
    hive> msck repair student_partition; # 修复表，显示"Repair: Added partition to metastore student_parition:month=202001"
```
    
- 手动增加分区：
```shell
    hive> dfs -mkdir -p /user/hive/warehouse/hive_test.db/student_partition/month=202002; # 创建目录
    hive> dfs -put /home/test/module/hive/data/student.txt /user/hive/warehouse/hive_test.db/student_partition/month=202002; # 上传文件
    hive> alter table student_partition add partition(month=202002); # 手动增加该分区
    hive> select * from student_partition where month='202002';
```

- 上传数据后load数据到分区
```shell
    hive> dfs -mkdir -p /user/hive/warehouse/hive_test.db/student_partition/month=202003; # 创建目录
    hive> load data local inpath '/home/test/module/hive/data/student.txt' into table student_partition partition(month=202003); # 上传数据
    hive> select * from student_partition where month='202003';
```


## 8. 修改表
### 8.1 重命名表
```hive> alter table student_parition rename to student_partition;```

### 8.2 增加、修改和删除表分区，见上文

### 8.3 增加、修改、替换列信息:
- 更新列: ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]
- 增加和替换列：ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...)
- 注意：ADD代表新增一个字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。
```shell
    示例：
    hive> alter table student_partition change column id unicodeId string; # 将int类型的id替换为string类型的unicodeId
    hive> alter table student_partition add columns (desc string, gender string); # 增加两列
    hive> alter table student_partition replace columns (id string, unicodeId string, name string, desc string); # 用新定义的列重新替换以前的列

```




